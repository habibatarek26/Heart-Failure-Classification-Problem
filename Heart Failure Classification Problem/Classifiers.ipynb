{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.cm import rainbow\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the Heart Failure Prediction Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"C:\\\\Users\\\\antoi\\\\.cache\\\\kagglehub\\\\datasets\\\\fedesoriano\\\\heart-failure-prediction\\\\versions\\\\1\\heart.csv\")\n",
    "dataset.head() \n",
    "\n",
    "# import os\n",
    "# import zipfile\n",
    "# # Download the dataset\n",
    "# path = kagglehub.dataset_download(\"fedesoriano/heart-failure-prediction\")\n",
    "\n",
    "# # Check if the dataset is a ZIP file\n",
    "# if path.endswith('.zip'):\n",
    "#     # Extract the ZIP file\n",
    "#     with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(\"extracted_dataset\")\n",
    "    \n",
    "#     # Find the CSV file in the extracted folder\n",
    "#     extracted_folder = \"extracted_dataset\"\n",
    "#     csv_file = [f for f in os.listdir(extracted_folder) if f.endswith('.csv')][0]\n",
    "#     csv_file_path = os.path.join(extracted_folder, csv_file)\n",
    "# else:\n",
    "#     # If it's not a ZIP file, look for the CSV file directly\n",
    "#     csv_file = [f for f in os.listdir(path) if f.endswith('.csv')][0]\n",
    "#     csv_file_path = os.path.join(path, csv_file)\n",
    "\n",
    "# # Load the dataset with pandas\n",
    "# dataset = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Display the dataset\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting a random seed ensures that any random processes produce the same results each time the code is run.\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=\"HeartDisease\")  # All columns except the target --> feature matrix\n",
    "y = dataset[\"HeartDisease\"]               # Target column --> target vector\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()   # scale features between 0 and 1 -> (x - min) / (max - min)\n",
    "# scalar = StandardScaler() # scale features to have mean=0 and variance=1 -> (x - mean) / std\n",
    "\n",
    "# Normalize only numeric columns\n",
    "for col in X.columns:\n",
    "    if pd.api.types.is_numeric_dtype(X[col]):\n",
    "        X[col] = scaler.fit_transform(X[[col]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features one-hot encoding\n",
    "cat_cols = X.select_dtypes(include='object').columns\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_val, y_train, y_test_val = train_test_split(\n",
    "    X, y, \n",
    "    random_state=42,   # For reproducibility ------> no need for shuffling\n",
    "    test_size=0.3,     # 30% for testing and validation, 70% for training\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test_val, y_test_val, \n",
    "    random_state=42,   # For reproducibility ------> no need for shuffling\n",
    "    test_size=1/3,     # 20% for testing, 10% for validation of the whole dataset\n",
    "    stratify=y_test_val\n",
    ")\n",
    "\n",
    "# Ensuring Ratio between number of zeros and ones are the same for all sets\n",
    "print(\"Test Set: \")\n",
    "print(y_test.value_counts())\n",
    "print(\"Ratio = \", y_test.value_counts()[0]/y_test.value_counts()[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Validation Set: \")\n",
    "print(y_val.value_counts())\n",
    "print(\"Ratio = \", y_val.value_counts()[0]/y_val.value_counts()[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Training Set: \")\n",
    "print(y_train.value_counts())\n",
    "print(\"Ratio = \", y_train.value_counts()[0]/y_train.value_counts()[1])\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "print(len(X_train), len(X_test), len(X_val))\n",
    "print(len(X), len(y))\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Implementation\n",
    "\n",
    "Working with a range of classification algorithms from scratch:\n",
    "1. Decision Tree\n",
    "2. Bagging Ensemble\n",
    "3. AdaBoost Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature  # Index of the feature to split on\n",
    "        self.threshold = threshold  # Threshold value for splitting\n",
    "        self.left = left # Left child node\n",
    "        self.right = right # Right child node\n",
    "        self.value = value  # Value aw el Class label if the node is a leaf node\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None    # Root node\n",
    "    \n",
    "    # Calculate the entropy of a list of labels\n",
    "    def entropy(self, y):\n",
    "        label_count = np.bincount(y)  \n",
    "        probabilities = label_count / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    \n",
    "    # Calculate the information gain of a split\n",
    "    def information_gain(self, X_feature_column, y, threshold):\n",
    "        parent_entropy = self.entropy(y)\n",
    "        \n",
    "        # Split the data\n",
    "        left_indices = X_feature_column < threshold\n",
    "        right_indices = X_feature_column >= threshold\n",
    "        \n",
    "        # Ignore the split if it doesn't divide the data\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate the weighted avg entropy of the children\n",
    "        n, left_n, right_n = len(y), len(y[left_indices]), len(y[right_indices])\n",
    "        # n, left_n, right_n = len(y), sum(left_indices), sum(right_indices)\n",
    "        left_entropy = self.entropy(y[left_indices])\n",
    "        right_entropy = self.entropy(y[right_indices])\n",
    "        weighted_avg_child_entropy = (left_n / n) * left_entropy + (right_n / n) * right_entropy\n",
    "        \n",
    "        # Calculate the information gain\n",
    "        return parent_entropy - weighted_avg_child_entropy\n",
    "    \n",
    "    # Find the best feature and threshold to split on\n",
    "    def best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])   #try all unique values in the feature as thresholds\n",
    "            for threshold in thresholds:\n",
    "                gain = self.information_gain(X[:, feature], y, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    # Build the decision tree recursively\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))        # num of classes\n",
    "        \n",
    "        # Stopping conditions\n",
    "        if depth >= self.max_depth or n_samples < self.min_samples_split or n_labels == 1:\n",
    "            leaf_value = np.argmax(np.bincount(y))      #majority class label\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self.best_split(X, y)\n",
    "        \n",
    "        if best_feature is None or best_threshold is None:\n",
    "            leaf_value = np.argmax(np.bincount(y))\n",
    "            return Node(value=leaf_value) \n",
    "            # return Node(value=np.argmax(np.bincount(y)))\n",
    "        \n",
    "        # Split the data\n",
    "        left_indices = X[:, best_feature] < best_threshold\n",
    "        right_indices = X[:, best_feature] >= best_threshold\n",
    "        \n",
    "        left_child = self.build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_child = self.build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return Node(best_feature, best_threshold, left_child, right_child)\n",
    "    \n",
    "    # Train the decision tree\n",
    "    def fit(self, X, y):\n",
    "        # Convert Pandas DataFrame and Series to NumPy arrays if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.to_numpy()\n",
    "    \n",
    "        self.root = self.build_tree(X, y)\n",
    "    \n",
    "    # Predict the class of a single sample\n",
    "    def predict(self, x_sample, node=None):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        \n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x_sample[node.feature] < node.threshold:\n",
    "            return self.predict(x_sample, node.left)\n",
    "        else:\n",
    "            return self.predict(x_sample, node.right)\n",
    "        \n",
    "    # Predict the classes of multiple samples\n",
    "    def predict_all(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()  # Ensure X is a NumPy array\n",
    "        return np.array([self.predict(x_sample, self.root) for x_sample in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decision tree\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# dict_decision_tree = {}\n",
    "results_decision_tree = {}\n",
    "val_f1_score_list = []  # Initialize the list to store the results\n",
    "train_f1_score_list = []\n",
    "\n",
    "for depth in range(2, 16, 1):\n",
    "    train_accuracy_decision_tree, val_accuracy_decision_tree = [], []\n",
    "    train_f1_score_decision_tree, val_f1_score_decision_tree = [], []\n",
    "    \n",
    "    for min_samples in range(2, 11, 2):\n",
    "        y_pred_train = None\n",
    "        y_pred_val = None\n",
    "        tree = DecisionTree(max_depth=depth, min_samples_split=min_samples)\n",
    "        tree.fit(X_train.values, y_train.values)\n",
    "        \n",
    "        # Predict the classes of the test set\n",
    "        y_pred_train = tree.predict_all(X_train.values)\n",
    "        train_accuracy = round(accuracy_score(y_train, y_pred_train) * 100, 3)\n",
    "        f1_score_train = round(f1_score(y_train, y_pred_train, average='macro') * 100, 3)\n",
    "        train_f1_score_decision_tree.append(f1_score_train)\n",
    "        train_accuracy_decision_tree.append(train_accuracy)\n",
    "        \n",
    "        # Predict the classes of the validation set\n",
    "        y_pred_val = tree.predict_all(X_val.values)\n",
    "        val_accuracy = round(accuracy_score(y_val, y_pred_val) * 100, 3)\n",
    "        f1_score_val = round(f1_score(y_val, y_pred_val, average='macro') * 100, 3)\n",
    "        val_f1_score_decision_tree.append(f1_score_val)\n",
    "        val_accuracy_decision_tree.append(val_accuracy)\n",
    "        \n",
    "        # Append the results to the list\n",
    "        val_f1_score_list.append({'Depth': depth, 'Min Samples': min_samples, 'Validation F1 Score': f1_score_val})\n",
    "        train_f1_score_list.append({'Depth': depth, 'Min Samples': min_samples, 'Train F1 Score': f1_score_train})\n",
    "    \n",
    "    results_decision_tree[depth] = {\n",
    "        \"train_accuracy\": train_accuracy_decision_tree,\n",
    "        \"val_accuracy\": val_accuracy_decision_tree,\n",
    "        \"train_f1_score\": train_f1_score_decision_tree,\n",
    "        \"val_f1_score\": val_f1_score_decision_tree,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "val_f1_score_df = pd.DataFrame(val_f1_score_list)\n",
    "\n",
    "# Pivot the DataFrame to represent depth as columns and min_samples as rows\n",
    "val_f1_score_pivot = val_f1_score_df.pivot(index='Min Samples', columns='Depth', values='Validation F1 Score')\n",
    "val_f1_score_pivot = val_f1_score_pivot.applymap(lambda x: f\"{x}%\")\n",
    "\n",
    "train_f1_score_df = pd.DataFrame(train_f1_score_list)\n",
    "train_f1_score_pivot = train_f1_score_df.pivot(index='Min Samples', columns='Depth', values='Train F1 Score')\n",
    "train_f1_score_pivot = train_f1_score_pivot.applymap(lambda x: f\"{x}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f1_score_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_f1_score_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Accuracy: \",results_decision_tree[5]['train_accuracy'][0], '%')\n",
    "print(\"Train F1 Score: \", results_decision_tree[5]['train_f1_score'][0], '%')\n",
    "print(\"Validation Accuracy: \", results_decision_tree[5]['val_accuracy'][0], '%')\n",
    "print(\"Validation F1 Score: \", results_decision_tree[5]['val_f1_score'][0], '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best hyperparameters\n",
    "\n",
    "best_depth = None\n",
    "best_min_samples = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for depth, metrics in results_decision_tree.items():\n",
    "    for i, val_acc in enumerate(metrics[\"val_accuracy\"]):\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_depth = depth\n",
    "            best_min_samples = i + 2  # since min_samples starts from 2\n",
    "\n",
    "print(f\"Best Hyperparameters: Depth = {best_depth} and Min Samples = {best_min_samples}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}%\")\n",
    "\n",
    "# Train the decision tree with the best hyperparameters\n",
    "tree = DecisionTree(max_depth=best_depth, min_samples_split=5)\n",
    "tree.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Predict the classes of the validation set\n",
    "# y_val_pred = tree.predict_all(X_val.values)\n",
    "\n",
    "# Calculate the F1 score for the validation set\n",
    "f1_score_val = f1_score(y_val, y_pred_val, average='macro')\n",
    "print(f\"Validation F1 Score: {f1_score_val * 100:.4f}%\")\n",
    "\n",
    "# Display the confusion matrix for the validation set\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(2)))\n",
    "disp.plot(cmap='viridis', values_format='d')\n",
    "plt.title(f\"Confusion Matrix for Validation Set with Depth = {best_depth}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Training Set Results, For Depth = \", best_depth, \" and Min Samples = \", best_min_samples)\n",
    "print(\"Train Accuracy = \", results_decision_tree[best_depth]['train_accuracy'][best_min_samples-2], \"%\")\n",
    "print(\"Train F1 Score = \", results_decision_tree[best_depth]['train_f1_score'][best_min_samples-2], \"%\")\n",
    "\n",
    "# Display the confusion matrix for the training set with the best hyperparameters\n",
    "y_train_pred = tree.predict_all(X_train.values)\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=list(range(2)))\n",
    "disp_train.plot(cmap='viridis', values_format='d')\n",
    "plt.title(f\"Confusion Matrix for Training Set with Depth = {best_depth}\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with the best hyperparameters\n",
    "test_tree = DecisionTree(max_depth=best_depth, min_samples_split=best_min_samples)\n",
    "test_tree.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Predict the classes of the test set with the best hyperparameters\n",
    "y_test_pred = test_tree.predict_all(X_test.values)\n",
    "\n",
    "# Computr test accuracy and F1 score on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "test_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test F1 Score: {test_f1_score * 100:.4f}%\")\n",
    "\n",
    "# Display the confusion matrix for the test set\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(2)))\n",
    "disp.plot(cmap='viridis', values_format='d')\n",
    "plt.title(f\"Confusion Matrix for Test Set with Depth = {best_depth}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Bagging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "class BaggingEnsemble:\n",
    " \n",
    "    #Create bootstrap samples randomly\n",
    "    def create_samples(self, X, y, num_of_resamples):\n",
    "        bootstrap_samples = []\n",
    "        for i in range(num_of_resamples):\n",
    "            resampled_X, resampled_y = resample(X, y, replace = True) \n",
    "            bootstrap_samples.append((resampled_X, resampled_y))\n",
    "        \n",
    "        return bootstrap_samples\n",
    "    \n",
    "    #obtain decision tree for each bootstrap\n",
    "    def train_models(self, bootstrap_samples):\n",
    "        base_learners = []\n",
    "        for resampled_X, resampled_y in bootstrap_samples:\n",
    "            decision_tree = DecisionTree(max_depth=5, min_samples_split=2)  # Using custom DecisionTree\n",
    "            decision_tree.fit(resampled_X, resampled_y)\n",
    "            base_learners.append(decision_tree)\n",
    "            \n",
    "        return base_learners\n",
    "    \n",
    "    # Calculate majority vote for predictions\n",
    "    def majority_predictions(self, base_learners, X):\n",
    "        # Each col represents the predictions of a certain sample and each row represents a certain decision tree from base_learners\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy() # Convert Pandas DataFrame to NumPy array if necessary\n",
    "        all_predictions = np.array([tree.predict_all(X) for tree in base_learners]) \n",
    "        final_prediction = np.apply_along_axis(lambda y: np.bincount(y).argmax(), axis=0, arr=all_predictions)\n",
    "         \n",
    "        return final_prediction\n",
    "    \n",
    "    \n",
    "# Lists to store results\n",
    "train_results = []\n",
    "val_results = []\n",
    "\n",
    "num_of_datasets = [10, 15, 20, 25, 30, 35]\n",
    "highest_accuracy = 0\n",
    "best_n_dataset = None\n",
    "best_base_learners = []\n",
    "bagging_ensemble = BaggingEnsemble()\n",
    "# Evaluate accuracy for different hyperparameter n using validation set\n",
    "for n in num_of_datasets:\n",
    "        \n",
    "    bootstrap_samples = bagging_ensemble.create_samples(X_train, y_train, n)\n",
    "    base_learners = bagging_ensemble.train_models(bootstrap_samples)\n",
    "    \n",
    "    predicted_y_training = bagging_ensemble.majority_predictions(base_learners, X_train)\n",
    "    training_accuracy = accuracy_score(y_train, predicted_y_training)\n",
    "    print(f\"Number of datasets = {n}, Training Accuracy = {training_accuracy}\")\n",
    "    \n",
    "    predicted_y_val = bagging_ensemble.majority_predictions(base_learners, X_val)\n",
    "    val_accuracy = accuracy_score(y_val, predicted_y_val)\n",
    "    print(f\"Number of datasets = {n}, Validation Accuracy = {val_accuracy}\\n\\n\")\n",
    "    \n",
    "    # Store results in lists\n",
    "    train_results.append({\"Number of Datasets\": n, \"Training Accuracy\": training_accuracy})\n",
    "    val_results.append({\"Number of Datasets\": n, \"Validation Accuracy\": val_accuracy})\n",
    "    \n",
    "    if val_accuracy > highest_accuracy:\n",
    "        highest_accuracy = val_accuracy\n",
    "        best_n_dataset = n\n",
    "        best_base_learners = base_learners\n",
    "               \n",
    "\n",
    "print(f\"\\nBest n estimators: {best_n_dataset} with Validation Accuracy: {highest_accuracy}\")  \n",
    "\n",
    "# Test the model\n",
    "predicted_y_test = bagging_ensemble.majority_predictions(best_base_learners, X_test)\n",
    "testing_accuracy = accuracy_score(y_test, predicted_y_test)\n",
    "print(f\"\\nTest accuracy with best n estimators {best_n_dataset} =  {testing_accuracy}\\n\") \n",
    "\n",
    "# Plot confusion matrix and display accuracy and F1 score\n",
    "print(\"Confusion Matrix for Training Set with Bagging Ensemble with best n_estimators = \", best_n_dataset)\n",
    "predicted_y_train = bagging_ensemble.majority_predictions(best_base_learners, X_train)\n",
    "print(\"Accuracy: \", accuracy_score(y_train, predicted_y_train))\n",
    "f1_score_train = f1_score(y_train, predicted_y_train)\n",
    "print(\"F1 Score: \", f1_score_train)\n",
    "cm = confusion_matrix(y_train, predicted_y_train)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(2)))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix for Validation Set with Bagging Ensemble with best n_estimators = \", best_n_dataset)\n",
    "print(\"Accuracy: \", highest_accuracy)\n",
    "predicted_y_validation = bagging_ensemble.majority_predictions(best_base_learners, X_val)\n",
    "f1_score_validation = f1_score(y_val, predicted_y_validation)\n",
    "print(\"F1 Score: \", f1_score_validation)\n",
    "cm = confusion_matrix(y_val, predicted_y_validation)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(2)))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix for Test Set with Bagging Ensemble with best n_estimators = \", best_n_dataset)\n",
    "print(\"Accuracy: \", testing_accuracy)\n",
    "f1_score_test = f1_score(y_test, predicted_y_test)\n",
    "print(\"F1 Score: \", f1_score_test)\n",
    "cm = confusion_matrix(y_test, predicted_y_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(2)))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert lists to DataFrames\n",
    "train_df_accuracy = pd.DataFrame(train_results)\n",
    "val_df_accuracy = pd.DataFrame(val_results)\n",
    "print(\"Training Accuracy DataFrame:\")\n",
    "print(train_df_accuracy)\n",
    "\n",
    "print(\"\\nValidation Accuracy DataFrame:\")\n",
    "print(val_df_accuracy) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: AdaBoost Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "class AdaBoost:\n",
    "\n",
    "    def __init__(self, X_train, X_test, X_val, y_train, y_test, y_val):\n",
    "        # Initialize datasets\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.X_val = X_val\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.y_val = y_val\n",
    "\n",
    "        # Initialize weights uniformly across all training samples\n",
    "        self.weights = np.ones(len(y_train)) / len(y_train)\n",
    "\n",
    "        # Store alpha values (model contributions) and weak learners\n",
    "        self.alphaArr = []\n",
    "        self.decision_trees = []\n",
    "\n",
    "    def error(self, predicted, y):\n",
    "        \"\"\" Calculate the weighted error (sum of weights for misclassified points). \"\"\"\n",
    "        return np.sum(self.weights * (predicted != y)) / np.sum(self.weights)\n",
    "\n",
    "    def alpha(self, errorVal):\n",
    "        \"\"\" Calculate the model's contribution (alpha) using the error. \"\"\"\n",
    "        # Clip error to avoid division by zero or taking log(0)\n",
    "        errorVal = np.clip(errorVal, 1e-10, 1 - 1e-10)\n",
    "        return 0.5 * np.log((1 - errorVal) / errorVal)\n",
    "\n",
    "    def reweight(self, predicted, alphaVal):\n",
    "        \"\"\" Update weights to focus on misclassified samples. \"\"\"\n",
    "        # Misclassified points get higher weights\n",
    "        self.weights *= np.exp(-1 * alphaVal * np.where(predicted != self.y_train, -1, 1))\n",
    "\n",
    "        # Normalize weights to sum to 1\n",
    "        self.weights /= np.sum(self.weights)\n",
    "\n",
    "    def _train_(self):\n",
    "        \"\"\" Train a weak learner (decision stump) for one iteration. \"\"\"\n",
    "        # Create and fit a decision tree with max_depth=1 (stump)\n",
    "        dtree = DecisionTreeClassifier(max_depth=1)\n",
    "        dtree.fit(self.X_train, self.y_train, sample_weight=self.weights)\n",
    "\n",
    "        # Predict on the training set\n",
    "        prediction = dtree.predict(self.X_train)\n",
    "\n",
    "        # Calculate weighted error\n",
    "        err = self.error(prediction, self.y_train)\n",
    "\n",
    "        # Only add weak learners with error < 0.5\n",
    "        if err < 0.5:\n",
    "            # Compute the model's contribution (alpha)\n",
    "            alphaVal = self.alpha(err)\n",
    "\n",
    "            # Store the model and its alpha value\n",
    "            self.decision_trees.append(dtree)\n",
    "            self.alphaArr.append(alphaVal)\n",
    "\n",
    "            # Update sample weights\n",
    "            self.reweight(prediction, alphaVal)\n",
    "\n",
    "        return prediction, err\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" Train the AdaBoost ensemble model. \"\"\"\n",
    "        # Initialize variables\n",
    "        final_res = np.zeros(len(self.y_train))\n",
    "        iteration_max = 100\n",
    "        error_diff_threshold = 0.05\n",
    "        overfit_count = 0\n",
    "        overfit_limit = 5\n",
    "\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "\n",
    "        for i in range(iteration_max):\n",
    "\n",
    "            # Train a weak learner\n",
    "            prediction, err = self._train_()\n",
    "\n",
    "            # Stop if error exceeds or equals 0.5\n",
    "            if err >= 0.5:\n",
    "                print(f\"Stopping early at iteration {i} (error={err:.4f})\")\n",
    "                break\n",
    "\n",
    "            # Calculate training error\n",
    "            trainError = np.sum(prediction != self.y_train) / len(self.y_train)\n",
    "\n",
    "            # Map predictions to {-1, 1} for AdaBoost math\n",
    "            prediction = np.where(prediction == 0, -1, 1)\n",
    "            final_res += self.alphaArr[i] * prediction\n",
    "\n",
    "            # Store training error for visualization\n",
    "            train_errors.append(trainError)\n",
    "\n",
    "            # Stop if perfect fit is achieved\n",
    "            if err == 0:\n",
    "                print(f\"Perfect fit at iteration {i}.\")\n",
    "                break\n",
    "\n",
    "            # Validate on validation set\n",
    "            val = self.validate()\n",
    "            val_errors.append(val)\n",
    "\n",
    "            # Check for overfitting based on error difference\n",
    "            if abs(val - trainError) > error_diff_threshold:\n",
    "                overfit_count += 1\n",
    "            else:\n",
    "                overfit_count = 0\n",
    "\n",
    "            # Handle overfitting by trimming models\n",
    "            if overfit_count >= overfit_limit:\n",
    "                print(f\"Detected Overfitting at iteration {i}.\")\n",
    "                print(f\"Just keep {i-overfit_limit} weak learners.\")\n",
    "\n",
    "                # Remove last overfit_limit models\n",
    "                if len(self.decision_trees) >= overfit_limit:\n",
    "                    self.decision_trees = self.decision_trees[:-overfit_limit]\n",
    "                    self.alphaArr = self.alphaArr[:-overfit_limit]\n",
    "\n",
    "                    # Recompute final result\n",
    "                    final_res = np.zeros(len(self.y_train))\n",
    "                    for j in range(len(self.decision_trees)):\n",
    "                        prediction = self.decision_trees[j].predict(self.X_train)\n",
    "                        prediction = np.where(prediction == 0, -1, 1)\n",
    "                        final_res += self.alphaArr[j] * prediction\n",
    "\n",
    "                break\n",
    "\n",
    "        # Plot training vs validation error\n",
    "        plt.plot(range(len(train_errors)), train_errors, label='Training Error')\n",
    "        plt.plot(range(len(val_errors)), val_errors, label='Validation Error')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Error')\n",
    "        plt.title('Training vs Validation Error')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        return np.where(final_res > 0, 1, 0)\n",
    "\n",
    "    def validateOrTest(self, x_test_val, y_test_val):\n",
    "        \"\"\" Predict using the ensemble of weak learners. \"\"\"\n",
    "        final_res = np.zeros(len(y_test_val))\n",
    "\n",
    "        for i in range(len(self.decision_trees)):\n",
    "            prediction = self.decision_trees[i].predict(x_test_val)\n",
    "            prediction = np.where(prediction == 0, -1, 1)\n",
    "            final_res += self.alphaArr[i] * prediction\n",
    "\n",
    "        return np.where(final_res > 0, 1, 0)\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\" Evaluate the model on the test set. \"\"\"\n",
    "        prediction = self.validateOrTest(self.X_test, self.y_test)\n",
    "        return np.sum(prediction != self.y_test) / len(self.y_test)\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\" Evaluate the model on the validation set. \"\"\"\n",
    "        prediction = self.validateOrTest(self.X_val, self.y_val)\n",
    "        return np.sum(prediction != self.y_val) / len(self.y_val)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Predict a single sample. \"\"\"\n",
    "        final_res = 0\n",
    "\n",
    "        for i in range(len(self.decision_trees)):\n",
    "            prediction = self.decision_trees[i].predict(x)\n",
    "            prediction = np.where(prediction == 0, -1, 1)\n",
    "            final_res += self.alphaArr[i] * prediction\n",
    "\n",
    "        return 1 if final_res > 0 else 0\n",
    "\n",
    "    def evaluate(self, x, y, dataset_name=\"Test\"):\n",
    "        \"\"\" Evaluate the model on a given dataset and display accuracy, F1-score, and confusion matrix. \"\"\"\n",
    "        # Get predictions\n",
    "        predictions = self.validateOrTest(x, y)\n",
    "\n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y, predictions)\n",
    "        f1 = f1_score(y, predictions)\n",
    "        cm = confusion_matrix(y, predictions)\n",
    "\n",
    "        print(f\"{dataset_name} Set Evaluation:\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\\n\")\n",
    "\n",
    "        # Display confusion matrix\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(f\"{dataset_name} Set Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def full_evaluation(self):\n",
    "        \"\"\" Evaluate the model on training, validation, and test sets. \"\"\"\n",
    "        self.evaluate(self.X_train, self.y_train, \"Training\")\n",
    "        self.evaluate(self.X_val, self.y_val, \"Validation\")\n",
    "        self.evaluate(self.X_test, self.y_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoost(X_train, X_test, X_val, y_train, y_test, y_val )\n",
    "tr =model.train()\n",
    "t =model.test()\n",
    "v = model.validate()\n",
    "model.full_evaluation()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"to be done\"\"\"\n",
    "\"\"\"done\"\"\"\n",
    "# need to put a fixed threshold on the diff between the tr error and val error  to detect overfitting\n",
    "# put limit on the max number of iterations  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame\n",
    "sample = pd.DataFrame({\n",
    "    'Age': [0.37],\n",
    "    'RestingBP': [0.80],\n",
    "    'Cholesterol': [0.45],\n",
    "    'FastingBS': [0.0],\n",
    "    'MaxHR': [0.68],\n",
    "    'Oldpeak': [0.30],\n",
    "    'Sex_M': [True],\n",
    "    'ChestPainType_ATA': [False],\n",
    "    'ChestPainType_NAP': [True],\n",
    "    'ChestPainType_TA': [False],\n",
    "    'RestingECG_Normal': [True],\n",
    "    'RestingECG_ST': [False],\n",
    "    'ExerciseAngina_Y': [False],\n",
    "    'ST_Slope_Flat': [False],\n",
    "    'ST_Slope_Up': [True]\n",
    "})\n",
    "\n",
    "# Display the sample\n",
    "print(sample)\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "prediction = model.predict(sample)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus Part: Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Train a logistic regression model with default parameters\n",
    "log_reg = LogisticRegression(max_iter=1000)  # Increase max_iter to ensure convergence\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Evaluate on the validation set\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy (Default Parameters): {val_accuracy:.2f}\")\n",
    "\n",
    "# Step 3: Tune the regularization parameter C using the validation set\n",
    "best_C = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Try different values of C\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for C in C_values:\n",
    "    # Train the model with the current C value\n",
    "    log_reg = LogisticRegression(C=C, max_iter=1000)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = log_reg.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"C = {C}, Validation Accuracy = {val_accuracy:.2f}\")\n",
    "    \n",
    "    # Track the best C value\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_C = C\n",
    "\n",
    "print(f\"\\nBest C: {best_C} with Validation Accuracy: {best_accuracy:.2f}\")\n",
    "\n",
    "# Step 4: Train the final model with the best C value on the full training set\n",
    "final_log_reg = LogisticRegression(C=best_C, max_iter=1000)\n",
    "final_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate on the test set\n",
    "y_test_pred = final_log_reg.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy with Best C ({best_C}): {test_accuracy:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
