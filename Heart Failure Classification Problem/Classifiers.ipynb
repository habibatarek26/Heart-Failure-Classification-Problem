{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.cm import rainbow\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the Heart Failure Prediction Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>45</td>\n",
       "      <td>M</td>\n",
       "      <td>TA</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>132</td>\n",
       "      <td>N</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>68</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>141</td>\n",
       "      <td>N</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>115</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>57</td>\n",
       "      <td>F</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>LVH</td>\n",
       "      <td>174</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>38</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>173</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>918 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n",
       "0     40   M           ATA        140          289          0     Normal   \n",
       "1     49   F           NAP        160          180          0     Normal   \n",
       "2     37   M           ATA        130          283          0         ST   \n",
       "3     48   F           ASY        138          214          0     Normal   \n",
       "4     54   M           NAP        150          195          0     Normal   \n",
       "..   ...  ..           ...        ...          ...        ...        ...   \n",
       "913   45   M            TA        110          264          0     Normal   \n",
       "914   68   M           ASY        144          193          1     Normal   \n",
       "915   57   M           ASY        130          131          0     Normal   \n",
       "916   57   F           ATA        130          236          0        LVH   \n",
       "917   38   M           NAP        138          175          0     Normal   \n",
       "\n",
       "     MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0      172              N      0.0       Up             0  \n",
       "1      156              N      1.0     Flat             1  \n",
       "2       98              N      0.0       Up             0  \n",
       "3      108              Y      1.5     Flat             1  \n",
       "4      122              N      0.0       Up             0  \n",
       "..     ...            ...      ...      ...           ...  \n",
       "913    132              N      1.2     Flat             1  \n",
       "914    141              N      3.4     Flat             1  \n",
       "915    115              Y      1.2     Flat             1  \n",
       "916    174              N      0.0     Flat             1  \n",
       "917    173              N      0.0       Up             0  \n",
       "\n",
       "[918 rows x 12 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "# Download the dataset\n",
    "path = kagglehub.dataset_download(\"fedesoriano/heart-failure-prediction\")\n",
    "\n",
    "# Check if the dataset is a ZIP file\n",
    "if path.endswith('.zip'):\n",
    "    # Extract the ZIP file\n",
    "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"extracted_dataset\")\n",
    "    \n",
    "    # Find the CSV file in the extracted folder\n",
    "    extracted_folder = \"extracted_dataset\"\n",
    "    csv_file = [f for f in os.listdir(extracted_folder) if f.endswith('.csv')][0]\n",
    "    csv_file_path = os.path.join(extracted_folder, csv_file)\n",
    "else:\n",
    "    # If it's not a ZIP file, look for the CSV file directly\n",
    "    csv_file = [f for f in os.listdir(path) if f.endswith('.csv')][0]\n",
    "    csv_file_path = os.path.join(path, csv_file)\n",
    "\n",
    "# Load the dataset with pandas\n",
    "dataset = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting a random seed ensures that any random processes produce the same results each time the code is run.\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244898</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.479270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>N</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>N</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.183673</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.469320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST</td>\n",
       "      <td>0.267606</td>\n",
       "      <td>N</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.408163</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.354892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.530612</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.323383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>N</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.346939</td>\n",
       "      <td>M</td>\n",
       "      <td>TA</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.437811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>N</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.816327</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.320066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.570423</td>\n",
       "      <td>N</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.591837</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.217247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>0.591837</td>\n",
       "      <td>F</td>\n",
       "      <td>ATA</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.391376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LVH</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>N</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>0.204082</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.290216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.795775</td>\n",
       "      <td>N</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>918 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n",
       "0    0.244898   M           ATA       0.70     0.479270        0.0     Normal   \n",
       "1    0.428571   F           NAP       0.80     0.298507        0.0     Normal   \n",
       "2    0.183673   M           ATA       0.65     0.469320        0.0         ST   \n",
       "3    0.408163   F           ASY       0.69     0.354892        0.0     Normal   \n",
       "4    0.530612   M           NAP       0.75     0.323383        0.0     Normal   \n",
       "..        ...  ..           ...        ...          ...        ...        ...   \n",
       "913  0.346939   M            TA       0.55     0.437811        0.0     Normal   \n",
       "914  0.816327   M           ASY       0.72     0.320066        1.0     Normal   \n",
       "915  0.591837   M           ASY       0.65     0.217247        0.0     Normal   \n",
       "916  0.591837   F           ATA       0.65     0.391376        0.0        LVH   \n",
       "917  0.204082   M           NAP       0.69     0.290216        0.0     Normal   \n",
       "\n",
       "        MaxHR ExerciseAngina   Oldpeak ST_Slope  \n",
       "0    0.788732              N  0.295455       Up  \n",
       "1    0.676056              N  0.409091     Flat  \n",
       "2    0.267606              N  0.295455       Up  \n",
       "3    0.338028              Y  0.465909     Flat  \n",
       "4    0.436620              N  0.295455       Up  \n",
       "..        ...            ...       ...      ...  \n",
       "913  0.507042              N  0.431818     Flat  \n",
       "914  0.570423              N  0.681818     Flat  \n",
       "915  0.387324              Y  0.431818     Flat  \n",
       "916  0.802817              N  0.295455     Flat  \n",
       "917  0.795775              N  0.295455       Up  \n",
       "\n",
       "[918 rows x 11 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.drop(columns=\"HeartDisease\")  # All columns except the target --> feature matrix\n",
    "y = dataset[\"HeartDisease\"]               # Target column --> target vector\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()   # scale features between 0 and 1 -> (x - min) / (max - min)\n",
    "\n",
    "# Normalize only numeric columns\n",
    "for col in X.columns:\n",
    "    if pd.api.types.is_numeric_dtype(X[col]):\n",
    "        X[col] = scaler.fit_transform(X[[col]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age  RestingBP  Cholesterol  FastingBS     MaxHR   Oldpeak  Sex_M  \\\n",
      "0  0.244898       0.70     0.479270        0.0  0.788732  0.295455      1   \n",
      "1  0.428571       0.80     0.298507        0.0  0.676056  0.409091      0   \n",
      "2  0.183673       0.65     0.469320        0.0  0.267606  0.295455      1   \n",
      "3  0.408163       0.69     0.354892        0.0  0.338028  0.465909      0   \n",
      "4  0.530612       0.75     0.323383        0.0  0.436620  0.295455      1   \n",
      "\n",
      "   ChestPainType_ATA  ChestPainType_NAP  ChestPainType_TA  RestingECG_Normal  \\\n",
      "0                  1                  0                 0                  1   \n",
      "1                  0                  1                 0                  1   \n",
      "2                  1                  0                 0                  0   \n",
      "3                  0                  0                 0                  1   \n",
      "4                  0                  1                 0                  1   \n",
      "\n",
      "   RestingECG_ST  ExerciseAngina_Y  ST_Slope_Flat  ST_Slope_Up  \n",
      "0              0                 0              0            1  \n",
      "1              0                 0              1            0  \n",
      "2              1                 0              0            1  \n",
      "3              0                 1              1            0  \n",
      "4              0                 0              0            1  \n"
     ]
    }
   ],
   "source": [
    "# Categorical features one-hot encoding\n",
    "cat_cols = X.select_dtypes(include='object').columns\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: \n",
      "1    102\n",
      "0     82\n",
      "Name: HeartDisease, dtype: int64\n",
      "Ratio =  0.803921568627451\n",
      "---------------------------------------\n",
      "Validation Set: \n",
      "1    51\n",
      "0    41\n",
      "Name: HeartDisease, dtype: int64\n",
      "Ratio =  0.803921568627451\n",
      "---------------------------------------\n",
      "Training Set: \n",
      "1    355\n",
      "0    287\n",
      "Name: HeartDisease, dtype: int64\n",
      "Ratio =  0.8084507042253521\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test_val, y_train, y_test_val = train_test_split(\n",
    "    X, y, \n",
    "    random_state=42,   # For reproducibility ------> no need for shuffling\n",
    "    test_size=0.3,     # 30% for testing and validation, 70% for training\n",
    "     stratify=y\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test_val, y_test_val, \n",
    "    random_state=42,   # For reproducibility ------> no need for shuffling\n",
    "    test_size=1/3,     # 20% for testing, 10% for validation of the whole dataset\n",
    "     stratify=y_test_val\n",
    ")\n",
    "\n",
    "# Ensuring Ratio between number of zeros and ones are the same for all sets\n",
    "print(\"Test Set: \")\n",
    "print(y_test.value_counts())\n",
    "print(\"Ratio = \", y_test.value_counts()[0]/y_test.value_counts()[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Validation Set: \")\n",
    "print(y_val.value_counts())\n",
    "print(\"Ratio = \", y_val.value_counts()[0]/y_val.value_counts()[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Training Set: \")\n",
    "print(y_train.value_counts())\n",
    "print(\"Ratio = \", y_train.value_counts()[0]/y_train.value_counts()[1])\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Implementation\n",
    "\n",
    "Working with a range of classification algorithms from scratch:\n",
    "1. Decision Tree\n",
    "2. Bagging Ensemble\n",
    "3. AdaBoost Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature  # Index of the feature to split on\n",
    "        self.threshold = threshold  # Threshold value for splitting\n",
    "        self.left = left # Left child node\n",
    "        self.right = right # Right child node\n",
    "        self.value = value  # Value aw el Class label if the node is a leaf node\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None    # Root node\n",
    "    \n",
    "    # Calculate the entropy of a list of labels\n",
    "    def entropy(self, y):\n",
    "        label_count = np.bincount(y) \n",
    "        probabilities = label_count / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    \n",
    "    # Calculate the information gain of a split\n",
    "    def information_gain(self, X_feature_column, y, threshold):\n",
    "        parent_entropy = self.entropy(y)\n",
    "        \n",
    "        # Split the data\n",
    "        left_indices = X_feature_column < threshold\n",
    "        right_indices = X_feature_column >= threshold\n",
    "        \n",
    "        # Ignore the split if it doesn't divide the data\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate the weighted avg entropy of the children\n",
    "        n, left_n, right_n = len(y), len(y[left_indices]), len(y[right_indices])\n",
    "        # n, left_n, right_n = len(y), sum(left_indices), sum(right_indices)\n",
    "        left_entropy = self.entropy(y[left_indices])\n",
    "        right_entropy = self.entropy(y[right_indices])\n",
    "        weighted_avg_child_entropy = (left_n / n) * left_entropy + (right_n / n) * right_entropy\n",
    "        \n",
    "        # Calculate the information gain\n",
    "        return parent_entropy - weighted_avg_child_entropy\n",
    "    \n",
    "    # Find the best feature and threshold to split on\n",
    "    def best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])   #try all unique values in the feature as thresholds\n",
    "            for threshold in thresholds:\n",
    "                gain = self.information_gain(X[:, feature], y, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    # Build the decision tree recursively\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))        # num of classes\n",
    "        \n",
    "        # Stopping conditions\n",
    "        if depth >= self.max_depth or n_samples < self.min_samples_split or n_labels == 1:\n",
    "            leaf_value = np.argmax(np.bincount(y))      #majority class label\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self.best_split(X, y)\n",
    "        \n",
    "        if best_feature is None or best_threshold is None:\n",
    "            leaf_value = np.argmax(np.bincount(y))\n",
    "            return Node(value=leaf_value) \n",
    "            # return Node(value=np.argmax(np.bincount(y)))\n",
    "        \n",
    "        # Split the data\n",
    "        left_indices = X[:, best_feature] < best_threshold\n",
    "        right_indices = X[:, best_feature] >= best_threshold\n",
    "        \n",
    "        left_child = self.build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_child = self.build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return Node(best_feature, best_threshold, left_child, right_child)\n",
    "    \n",
    "    # Train the decision tree\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "        # print(f\"Root node: {vars(self.root)}\")\n",
    "    \n",
    "    # Predict the class of a single sample\n",
    "    def predict(self, x_sample, node=None):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        \n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x_sample[node.feature] < node.threshold:\n",
    "            return self.predict(x_sample, node.left)\n",
    "        else:\n",
    "            return self.predict(x_sample, node.right)\n",
    "        \n",
    "    # Predict the classes of multiple samples\n",
    "    def predict_all(self, X):\n",
    "        return np.array([self.predict(x_sample, self.root) for x_sample in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.1522%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81        82\n",
      "           1       0.84      0.86      0.85       102\n",
      "\n",
      "    accuracy                           0.83       184\n",
      "   macro avg       0.83      0.83      0.83       184\n",
      "weighted avg       0.83      0.83      0.83       184\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[65 17]\n",
      " [14 88]]\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree\n",
    "tree = DecisionTree(max_depth=8, min_samples_split=2)\n",
    "tree.fit(X_train.values, y_train.values)    \n",
    "\n",
    "# Predict the classes of the test set\n",
    "y_pred = tree.predict_all(X_test.values)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 3, min_samples_split: 2, Validation Accuracy: 0.8261\n",
      "max_depth: 3, min_samples_split: 3, Validation Accuracy: 0.8261\n",
      "max_depth: 3, min_samples_split: 4, Validation Accuracy: 0.8261\n",
      "max_depth: 3, min_samples_split: 5, Validation Accuracy: 0.8261\n",
      "max_depth: 3, min_samples_split: 6, Validation Accuracy: 0.8261\n",
      "max_depth: 3, min_samples_split: 7, Validation Accuracy: 0.8261\n",
      "max_depth: 3, min_samples_split: 8, Validation Accuracy: 0.8261\n",
      "max_depth: 3, min_samples_split: 9, Validation Accuracy: 0.8261\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 4, min_samples_split: 2, Validation Accuracy: 0.8370\n",
      "max_depth: 4, min_samples_split: 3, Validation Accuracy: 0.8370\n",
      "max_depth: 4, min_samples_split: 4, Validation Accuracy: 0.8370\n",
      "max_depth: 4, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 4, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 4, min_samples_split: 7, Validation Accuracy: 0.8370\n",
      "max_depth: 4, min_samples_split: 8, Validation Accuracy: 0.8370\n",
      "max_depth: 4, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 5, min_samples_split: 2, Validation Accuracy: 0.8478\n",
      "max_depth: 5, min_samples_split: 3, Validation Accuracy: 0.8478\n",
      "max_depth: 5, min_samples_split: 4, Validation Accuracy: 0.8478\n",
      "max_depth: 5, min_samples_split: 5, Validation Accuracy: 0.8478\n",
      "max_depth: 5, min_samples_split: 6, Validation Accuracy: 0.8478\n",
      "max_depth: 5, min_samples_split: 7, Validation Accuracy: 0.8478\n",
      "max_depth: 5, min_samples_split: 8, Validation Accuracy: 0.8478\n",
      "max_depth: 5, min_samples_split: 9, Validation Accuracy: 0.8478\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 6, min_samples_split: 2, Validation Accuracy: 0.8370\n",
      "max_depth: 6, min_samples_split: 3, Validation Accuracy: 0.8370\n",
      "max_depth: 6, min_samples_split: 4, Validation Accuracy: 0.8370\n",
      "max_depth: 6, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 6, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 6, min_samples_split: 7, Validation Accuracy: 0.8370\n",
      "max_depth: 6, min_samples_split: 8, Validation Accuracy: 0.8370\n",
      "max_depth: 6, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 7, min_samples_split: 2, Validation Accuracy: 0.8370\n",
      "max_depth: 7, min_samples_split: 3, Validation Accuracy: 0.8370\n",
      "max_depth: 7, min_samples_split: 4, Validation Accuracy: 0.8370\n",
      "max_depth: 7, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 7, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 7, min_samples_split: 7, Validation Accuracy: 0.8370\n",
      "max_depth: 7, min_samples_split: 8, Validation Accuracy: 0.8370\n",
      "max_depth: 7, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 8, min_samples_split: 2, Validation Accuracy: 0.8370\n",
      "max_depth: 8, min_samples_split: 3, Validation Accuracy: 0.8370\n",
      "max_depth: 8, min_samples_split: 4, Validation Accuracy: 0.8370\n",
      "max_depth: 8, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 8, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 8, min_samples_split: 7, Validation Accuracy: 0.8261\n",
      "max_depth: 8, min_samples_split: 8, Validation Accuracy: 0.8261\n",
      "max_depth: 8, min_samples_split: 9, Validation Accuracy: 0.8261\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 9, min_samples_split: 2, Validation Accuracy: 0.8478\n",
      "max_depth: 9, min_samples_split: 3, Validation Accuracy: 0.8478\n",
      "max_depth: 9, min_samples_split: 4, Validation Accuracy: 0.8478\n",
      "max_depth: 9, min_samples_split: 5, Validation Accuracy: 0.8478\n",
      "max_depth: 9, min_samples_split: 6, Validation Accuracy: 0.8478\n",
      "max_depth: 9, min_samples_split: 7, Validation Accuracy: 0.8370\n",
      "max_depth: 9, min_samples_split: 8, Validation Accuracy: 0.8370\n",
      "max_depth: 9, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 10, min_samples_split: 2, Validation Accuracy: 0.8370\n",
      "max_depth: 10, min_samples_split: 3, Validation Accuracy: 0.8370\n",
      "max_depth: 10, min_samples_split: 4, Validation Accuracy: 0.8370\n",
      "max_depth: 10, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 10, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 10, min_samples_split: 7, Validation Accuracy: 0.8261\n",
      "max_depth: 10, min_samples_split: 8, Validation Accuracy: 0.8261\n",
      "max_depth: 10, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 11, min_samples_split: 2, Validation Accuracy: 0.8478\n",
      "max_depth: 11, min_samples_split: 3, Validation Accuracy: 0.8478\n",
      "max_depth: 11, min_samples_split: 4, Validation Accuracy: 0.8478\n",
      "max_depth: 11, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 11, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 11, min_samples_split: 7, Validation Accuracy: 0.8261\n",
      "max_depth: 11, min_samples_split: 8, Validation Accuracy: 0.8261\n",
      "max_depth: 11, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 12, min_samples_split: 2, Validation Accuracy: 0.8478\n",
      "max_depth: 12, min_samples_split: 3, Validation Accuracy: 0.8478\n",
      "max_depth: 12, min_samples_split: 4, Validation Accuracy: 0.8478\n",
      "max_depth: 12, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 12, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 12, min_samples_split: 7, Validation Accuracy: 0.8261\n",
      "max_depth: 12, min_samples_split: 8, Validation Accuracy: 0.8261\n",
      "max_depth: 12, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 13, min_samples_split: 2, Validation Accuracy: 0.8478\n",
      "max_depth: 13, min_samples_split: 3, Validation Accuracy: 0.8478\n",
      "max_depth: 13, min_samples_split: 4, Validation Accuracy: 0.8478\n",
      "max_depth: 13, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 13, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 13, min_samples_split: 7, Validation Accuracy: 0.8261\n",
      "max_depth: 13, min_samples_split: 8, Validation Accuracy: 0.8261\n",
      "max_depth: 13, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "max_depth: 14, min_samples_split: 2, Validation Accuracy: 0.8478\n",
      "max_depth: 14, min_samples_split: 3, Validation Accuracy: 0.8478\n",
      "max_depth: 14, min_samples_split: 4, Validation Accuracy: 0.8478\n",
      "max_depth: 14, min_samples_split: 5, Validation Accuracy: 0.8370\n",
      "max_depth: 14, min_samples_split: 6, Validation Accuracy: 0.8370\n",
      "max_depth: 14, min_samples_split: 7, Validation Accuracy: 0.8261\n",
      "max_depth: 14, min_samples_split: 8, Validation Accuracy: 0.8261\n",
      "max_depth: 14, min_samples_split: 9, Validation Accuracy: 0.8370\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Best max_depth: 5, Best min_samples_split: 2, Best Validation Accuracy: 0.8478\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with the validation set\n",
    "best_depth = None\n",
    "best_min_samples_split = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for depth in range(3, 15):\n",
    "    for min_samples in range(2, 10):\n",
    "        tree = DecisionTree(max_depth=depth, min_samples_split=min_samples)\n",
    "        tree.fit(X_train.values, y_train.values)\n",
    "        y_val_pred = tree.predict_all(X_val.values)\n",
    "\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"max_depth: {depth}, min_samples_split: {min_samples}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_depth = depth\n",
    "            best_min_samples_split = min_samples\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(f\"Best max_depth: {best_depth}, Best min_samples_split: {best_min_samples_split}, Best Validation Accuracy: {best_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
